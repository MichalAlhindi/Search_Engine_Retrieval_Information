{"cells":[{"cell_type":"markdown","metadata":{"id":"Mw0dCzWAmPP4"},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"MD7myMAv_-9I","outputId":"c346a2a4-d1cb-4075-df93-626386e23ad0"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME            PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","sean-cluster-7  GCE       4                                       RUNNING  us-central1-a\r\n"]}],"source":["!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bN7FjhC3AC6B","outputId":"6d396b01-d32d-4316-8b43-7b5a28f6aae3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"bJGbo-FD_8h0","outputId":"b0274a52-3d00-4a23-abfd-e8733f988e38","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["%cd -q /home/dataproc\n","!ls inverted_index_gcp.py\n","from inverted_index_gcp import InvertedIndex"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQrL89eU8vzb","outputId":"e6c1ba1a-e992-46f7-e8a0-4824b5626789"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mPackage openjdk-8-jdk-headless is not available, but is referred to by another package.\n","This may mean that the package is missing, has been obsoleted, or\n","is only available from another source\n","\n","E: Package 'openjdk-8-jdk-headless' has no installation candidate\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# These will already be installed in the testing environment so disregard the \n","# amount of time (~1 minute) it takes to install. \n","!pip install -q pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","!pip install -q graphframes\n","\n","\n","\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import math\n","from functools import reduce\n","from google.cloud import storage\n","from inverted_index_gcp import *\n","\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","# ''' code addition'''\n","# from flask import Flask, request, jsonify\n","# ''' code addition'''\n","\n","nltk.download('stopwords')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ylhUZMJK-OG7"},"source":["# *PySpark*"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IG3wQJBa_8h3","outputId":"6b3f79d7-362c-40fe-daab-78911321b03d"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan  8 00:27 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"5G2JQbalBUSX"},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://sean-cluster-7-m.c.linear-rig-370208.internal:42249\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fc1eb8dec70>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"iwq6C8pvpphA"},"source":["# Index Functions"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0uJat8jHsFRD"},"outputs":[],"source":["# Calculating tf\n","def word_count_b(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    dict_counter= Counter(tokens)\n","    new_list=[(k, (id,dict_counter[k])) for k in dict_counter if k not in all_stopwords]\n","    return new_list\n","\n","# get list of (id, terms)\n","def doc_count_b(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    return [(id,tokens)]\n","    \n","# Calculating tf for title\n","def word_count_t(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    dict_counter = Counter(tokens)\n","    return [(k, (id,1)) for k in dict_counter if k not in all_stopwords]\n","\n","# Sort posting list by wiki_id\n","def reduce_word_counts(unsorted_pl): return sorted(unsorted_pl, key=lambda k: k[0])\n","\n","# Calculate df for each token in a posting list\n","def calculate_df(postings): return postings.map(lambda x: (x[0], len(x[1])))\n","\n","#Write to the disk all posting lists locations\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","def partition_postings_and_write(postings,bucket_name):\n","    rd = postings.map(lambda x : (token2bucket_id(x[0]),(x[0],x[1])))\n","    rd = rd.groupByKey()\n","    return rd.map(lambda x: InvertedIndex.write_a_posting_list(x,bucket_name))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3JFAqcDMqYYv"},"source":["# Suppot cluster"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hmOee5WoqWHr"},"outputs":[],"source":["# Stopwords\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","all_stopwords = english_stopwords.union(corpus_stopwords)"]},{"cell_type":"markdown","metadata":{"id":"JphMXTS7mSaf"},"source":["# Creating Index"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"8DcEJ-PSmVtb"},"outputs":[],"source":["def createIndex(bucket_name, index_name):\n","    paths=[]\n","    client = storage.Client()\n","    bucket_name_title = bucket_name\n","    full_path = f\"gs://{bucket_name}/\"\n","\n","    blobs = client.list_blobs(bucket_name)\n","    for b in blobs:\n","        if b.name.endswith(\"parquet\"):\n","            paths.append(full_path+b.name)\n","\n","    # Wikipidia\n","    parquetFile = spark.read.parquet(*paths)\n","    dict_title_id= parquetFile.select(\"id\", \"title\").rdd\n","    if index_name == \"anchor\":\n","      doc_pairs = parquetFile.select(\"id\",f\"{index_name}_text\").rdd \n","    else:\n","      doc_pairs = parquetFile.select(f\"{index_name}\", \"id\").rdd\n","\n","\n","    word_counts = doc_pairs.flatMap(lambda x: word_count_t(x[0], x[1]))\n","\n","    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","    w2df = calculate_df(postings)\n","\n","    w2df_dict = w2df.collectAsMap()\n","\n","    posting_locs_list = partition_postings_and_write(postings, bucket_name).collect()\n","\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","        if not blob.name.endswith(\"pickle\"):\n","            continue\n","        with blob.open(\"rb\") as f:\n","            posting_locs = pickle.load(f)\n","            for k, v in posting_locs.items():\n","                super_posting_locs[k].extend(v)\n","\n","    inverted = InvertedIndex()\n","    inverted.posting_locs=super_posting_locs\n","    inverted.df=w2df_dict\n","    inverted.title_dict = dict_title_id.collectAsMap()\n","\n","    inverted.write_index('.',  f'index_{index_name}')\n","    index_src =  f\"index_{index_name}.pkl\"\n","    index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","    !gsutil cp $index_src $index_dst\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Copying file://index_title.pkl [Content-Type=application/octet-stream]...\n","==> NOTE: You are uploading one or more large file(s), which would run          \n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","/ [1 files][236.5 MiB/236.5 MiB]                                                \n","Operation completed over 1 objects/236.5 MiB.                                    \n"]}],"source":["createIndex(\"sean_bucket_title\",\"title\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["createIndex(\"sean_bucket_body\",\"text\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["Mw0dCzWAmPP4","ylhUZMJK-OG7","iwq6C8pvpphA","3JFAqcDMqYYv"],"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":1}